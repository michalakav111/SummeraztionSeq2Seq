# -*- coding: utf-8 -*-
"""workProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z9WkT8h0RNS_pTYZ3fnbBNRTure1RI1t

חיבור ל drive
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/My Drive")

import tensorflow as tf
from tensorflow.python.keras import backend as K

logger = tf.get_logger()

class AttentionLayer(tf.keras.layers.Layer):
    """
    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).
    There are three sets of weights introduced W_a, U_a, and V_a
     """

    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        assert isinstance(input_shape, list)
        # Create a trainable weight variable for this layer.

        self.W_a = self.add_weight(name='W_a',
                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),
                                   initializer='uniform',
                                   trainable=True)
        self.U_a = self.add_weight(name='U_a',
                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),
                                   initializer='uniform',
                                   trainable=True)
        self.V_a = self.add_weight(name='V_a',
                                   shape=tf.TensorShape((input_shape[0][2], 1)),
                                   initializer='uniform',
                                   trainable=True)

        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end

    def call(self, inputs):
        """
        inputs: [encoder_output_sequence, decoder_output_sequence]
        """
        assert type(inputs) == list
        encoder_out_seq, decoder_out_seq = inputs

        logger.debug(f"encoder_out_seq.shape = {encoder_out_seq.shape}")
        logger.debug(f"decoder_out_seq.shape = {decoder_out_seq.shape}")

        def energy_step(inputs, states):
            """ Step function for computing energy for a single decoder state
            inputs: (batchsize * 1 * de_in_dim)
            states: (batchsize * 1 * de_latent_dim)
            """

            logger.debug("Running energy computation step")

            if not isinstance(states, (list, tuple)):
                raise TypeError(f"States must be an iterable. Got {states} of type {type(states)}")

            encoder_full_seq = states[-1]

            """ Computing S.Wa where S=[s0, s1, ..., si]"""
            # <= batch size * en_seq_len * latent_dim
            W_a_dot_s = K.dot(encoder_full_seq, self.W_a)

            """ Computing hj.Ua """
            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim

            logger.debug(f"U_a_dot_h.shape = {U_a_dot_h.shape}")

            """ tanh(S.Wa + hj.Ua) """
            # <= batch_size*en_seq_len, latent_dim
            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)

            logger.debug(f"Ws_plus_Uh.shape = {Ws_plus_Uh.shape}")

            """ softmax(va.tanh(S.Wa + hj.Ua)) """
            # <= batch_size, en_seq_len
            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)
            # <= batch_size, en_seq_len
            e_i = K.softmax(e_i)

            logger.debug(f"ei.shape = {e_i.shape}")

            return e_i, [e_i]

        def context_step(inputs, states):
            """ Step function for computing ci using ei """

            logger.debug("Running attention vector computation step")

            if not isinstance(states, (list, tuple)):
                raise TypeError(f"States must be an iterable. Got {states} of type {type(states)}")

            encoder_full_seq = states[-1]

            # <= batch_size, hidden_size
            c_i = K.sum(encoder_full_seq * K.expand_dims(inputs, -1), axis=1)

            logger.debug(f"ci.shape = {c_i.shape}")

            return c_i, [c_i]

        # we don't maintain states between steps when computing attention
        # attention is stateless, so we're passing a fake state for RNN step function
        fake_state_c = K.sum(encoder_out_seq, axis=1)
        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim

        """ Computing energy outputs """
        # e_outputs => (batch_size, de_seq_len, en_seq_len)
        last_out, e_outputs, _ = K.rnn(
            energy_step, decoder_out_seq, [fake_state_e], constants=[encoder_out_seq]
        )

        """ Computing context vectors """
        last_out, c_outputs, _ = K.rnn(
            context_step, e_outputs, [fake_state_c], constants=[encoder_out_seq]
        )

        return c_outputs, e_outputs

    def compute_output_shape(self, input_shape):
        """ Outputs produced by the layer """
        return [
            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),
            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))
        ]

"""יבוא סיפריות רצויות"""

import os

import tensorflow as tf
from tensorflow import keras

import numpy as np
import pandas as pd
import re
from bs4 import BeautifulSoup
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
import warnings

pd.set_option("display.max_colwidth", 200)
warnings.filterwarnings("ignore")

"""חיבור לDATA"""

data=pd.read_csv("./Reviews.csv")

"""עיבוד הDATA"""

#הורדת כפילויות
data.drop_duplicates(subset=['Text'],inplace=True)
#הורדת ערכים חסרים
data.dropna(axis=0,inplace=True)

#נוריד את כל הסמלים, התווים וכו' הלא רצויים מהטקסט שאינם משפיעים על מטרת הבעיה שלנו
contraction_mapping = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have", "couldn't": "could not",


	                           "didn't": "did not", "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",


	                           "he'd": "he would","he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",


	                           "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", "I've": "I have", "i'd": "i would",


	                           "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am", "i've": "i have", "isn't": "is not", "it'd": "it would",


	                           "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have","it's": "it is", "let's": "let us", "ma'am": "madam",


	                           "mayn't": "may not", "might've": "might have","mightn't": "might not","mightn't've": "might not have", "must've": "must have",


	                           "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have","o'clock": "of the clock",


	                           "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",


	                           "she'd": "she would", "she'd've": "she would have", "she'll": "she will", "she'll've": "she will have", "she's": "she is",


	                           "should've": "should have", "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have","so's": "so as",


	                           "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is", "there'd": "there would",


	                           "there'd've": "there would have", "there's": "there is", "here's": "here is","they'd": "they would", "they'd've": "they would have",


	                           "they'll": "they will", "they'll've": "they will have", "they're": "they are", "they've": "they have", "to've": "to have",


	                           "wasn't": "was not", "we'd": "we would", "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",


	                           "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have", "what're": "what are",


	                           "what's": "what is", "what've": "what have", "when's": "when is", "when've": "when have", "where'd": "where did", "where's": "where is",


	                           "where've": "where have", "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have",


	                           "why's": "why is", "why've": "why have", "will've": "will have", "won't": "will not", "won't've": "will not have",


	                           "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have", "y'all": "you all",


	                           "y'all'd": "you all would","y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have",


	                           "you'd": "you would", "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",


	                           "you're": "you are", "you've": "you have"}

import nltk
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
def text_cleaner(text):
    newString = text.lower()#הופך את כל הטקסט לאותיות קטנות
    newString = BeautifulSoup(newString, "html.parser").text#מוריד תגי HTML
    newString = re.sub(r'\([^)]*\)', '', newString)#מיפוי התכווצות
    newString = re.sub('"','', newString)#הסרת S
    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(" ")])#
    newString = re.sub(r"'s\b","",newString)
    newString = re.sub("[^a-zA-Z]", " ", newString)
    tokens = [w for w in newString.split() if not w in stop_words]
    long_words=[]
    for i in tokens:
        if len(i)>=3:                  #removing short word
            long_words.append(i)
    return (" ".join(long_words)).strip()

cleaned_text = []
for t in data['Text']:
    cleaned_text.append(text_cleaner(t))

data['Summary'][:10]

def summary_cleaner(text):
    newString = re.sub('"','', text)
    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(" ")])
    newString = re.sub(r"'s\b","",newString)
    newString = re.sub("[^a-zA-Z]", " ", newString)
    newString = newString.lower()
    tokens=newString.split()
    newString=''
    for i in tokens:
        if len(i)>1:
            newString=newString+i+' '
    return newString

#Call the above function
cleaned_summary = []
for t in data['Summary']:
    cleaned_summary.append(summary_cleaner(t))

data['Text'][:10]

data['cleaned_text']=cleaned_text
data['cleaned_summary']=cleaned_summary
data['cleaned_summary'].replace('', np.nan, inplace=True)
data.dropna(axis=0,inplace=True)

type(data)

# _START_ _END_בחלק של הסיכום מוסיפים את הטוקנים
data['cleaned_summary'] = data['cleaned_summary'].apply(lambda x : '_START_ '+ x + ' _END_')

for i in range(5):
    print("Review:",data['cleaned_text'][i])
    print("Summary:",data['cleaned_summary'][i])
    print("\n")

#בדיקת ההתפלגות והצגה בגרף

import matplotlib.pyplot as plt
text_word_count = []
summary_word_count = []

# populate the lists with sentence lengths
for i in data['cleaned_text']:
      text_word_count.append(len(i.split()))

for i in data['cleaned_summary']:
      summary_word_count.append(len(i.split()))

length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})
length_df.hist(bins = 30)
plt.show()

max_len_text=80
max_len_summary=10

#חילוק הנתונים לאימון ובדיקה  90%ה ו10%

from sklearn.model_selection import train_test_split
x_tr,x_val,y_tr,y_val=train_test_split(data['cleaned_text'],data['cleaned_summary'],test_size=0.1,random_state=0,shuffle=True)

"""פרוט קוד עבור x_tokenaizer"""

x_tokenizer = Tokenizer()
x_tokenizer.fit_on_texts(list(x_tr))

x_tr  = x_tokenizer.texts_to_sequences(x_tr)

x_tr[1]

x_val =  x_tokenizer.texts_to_sequences(x_val)

x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post')
x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')

x_tr[1]

x_voc_size   =  len(x_tokenizer.word_index) +1

type(x_tokenizer.word_index)

x_voc_size

"""בניית ה y_tokenaizer"""

y_tokenizer = Tokenizer()
y_tokenizer.fit_on_texts(list(y_tr))
y_tr    =   y_tokenizer.texts_to_sequences(y_tr)
y_val   =   y_tokenizer.texts_to_sequences(y_val)

y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')
y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')

y_voc_size  =   len(y_tokenizer.word_index) +1

# x_tokenizer = Tokenizer()
# x_tokenizer.fit_on_texts(list(x_tr))

# x_tr  = x_tokenizer.texts_to_sequences(x_tr)
# x_val =  x_tokenizer.texts_to_sequences(x_val)

# x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post')
# x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')

# x_voc_size   =  len(x_tokenizer.word_index) +1


# y_tokenizer = Tokenizer()
# y_tokenizer.fit_on_texts(list(y_tr))
# y_tr    =   y_tokenizer.texts_to_sequences(y_tr)
# y_val   =   y_tokenizer.texts_to_sequences(y_val)

# y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')
# y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')

# y_voc_size  =   len(y_tokenizer.word_index) +1

"""#בניית המודל הראשון ללא   Bidirectional LSTM

"""

# from keras import backend as K
# K.clear_session()
# latent_dim = 500

# # Encoder
# encoder_inputs = Input(shape=(max_len_text,))
# enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs)

# #LSTM 1
# encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True)
# encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)

# #LSTM 2
# encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True)
# encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)

# #LSTM 3
# encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True)
# encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)

# # Set up the decoder.
# decoder_inputs = Input(shape=(None,))
# dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True)
# dec_emb = dec_emb_layer(decoder_inputs)

# #LSTM using encoder_states as initial state
# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
# decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])

# #Attention Layer
# attn_layer = AttentionLayer(name='attention_layer')
# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])
# # Concat attention output and decoder LSTM output
# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])

# #Dense layer
# decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))
# decoder_outputs = decoder_dense(decoder_concat_input)

# # Define the model
# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
# model.summary()

"""#המודל הנבחר

"""

from keras.layers import Input, Embedding, LSTM, Bidirectional, Concatenate, Dense, TimeDistributed
from keras.layers import Attention
from keras.models import Model


K.clear_session()
latent_dim = 500

# Encoder
encoder_inputs = Input(shape=(max_len_text,))
enc_emb = Embedding(x_voc_size, latent_dim, trainable=True)(encoder_inputs)

# Bidirectional LSTM
encoder_bilstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True))
encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(enc_emb)
state_h = Concatenate()([forward_h, backward_h])
state_c = Concatenate()([forward_c, backward_c])

# Set up the decoder.
decoder_inputs = Input(shape=(None,))
dec_emb_layer = Embedding(y_voc_size, latent_dim, trainable=True)
dec_emb = dec_emb_layer(decoder_inputs)

# LSTM
decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])

# Attention Layer
attn_layer = AttentionLayer(name='attention_layer')
attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])

# Concat attention output and decoder LSTM output
decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])

# Dense layer
decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))
decoder_outputs = decoder_dense(decoder_concat_input)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.summary()

model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)

tf.keras.saving.get_custom_objects().clear()

tf.keras.utils.get_custom_objects()['AttentionLayer'] = AttentionLayer

history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=10,callbacks=[es],batch_size=512, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))

"""ניסיון חדש של שמירה של המודל כ2 מודלים של ENCODER וDECODER של הINFERENCE ואותם לטעון

# בניית מודל הINFERANCE
"""

# Encoder Inference
encoder = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])

# Decoder Inference
# Below tensors will hold the states of the previous time step
decoder_state_input_h = Input(shape=(latent_dim*2,))  # Update shape to accommodate BiLSTM
decoder_state_input_c = Input(shape=(latent_dim*2,))  # Update shape to accommodate BiLSTM
decoder_hidden_state_input = Input(shape=(max_len_text, latent_dim*2))  # Update shape to accommodate BiLSTM

# Get the embeddings of the decoder sequence
dec_emb2 = dec_emb_layer(decoder_inputs)

# To predict the next word in the sequence, set the initial states to the states from the previous time step
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])

# Attention Inference
attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])
decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])

# A dense softmax layer to generate probability distribution over the target vocabulary
decoder_outputs2 = decoder_dense(decoder_inf_concat)

# Final decoder model
decoder = Model(
    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],
    [decoder_outputs2] + [state_h2, state_c2])

"""#חלק מהמודלים שבניתי וניסיתי"""

# with open('TRYencoder_modelLilstm2.json', 'w', encoding='utf8') as f:
#     f.write(encoder.to_json())
# encoder.save_weights('TRYencoder_model_weightsLilstm2.h5')

# with open('TRYdecoder_modelLilstm2.json', 'w', encoding='utf8') as f:
#     f.write(decoder.to_json())
# decoder.save_weights('TRYdecoder_model_weightsLilstm2.h5')

# with open('encoder_modelLilstm2.json', 'w', encoding='utf8') as f:
#     f.write(encoder.to_json())
# encoder.save_weights('encoder_model_weightsLilstm2.h5')

# with open('decoder_modelLilstm2.json', 'w', encoding='utf8') as f:
#     f.write(decoder.to_json())
# decoder.save_weights('decoder_model_weightsLilstm2.h5')

with open('encoder_modelLilstm.json', 'w', encoding='utf8') as f:
    f.write(encoder.to_json())
encoder.save_weights('encoder_model_weightsLilstm.h5')

with open('decoder_modelLilstm.json', 'w', encoding='utf8') as f:
    f.write(decoder.to_json())
decoder.save_weights('decoder_model_weightsLilstm.h5')

"""#טעינת המודל"""

import tensorflow as tf
from tensorflow.keras.models import model_from_json

def load_model(model_filename, model_weights_filename):
    with open(model_filename, 'r', encoding='utf8') as f:
        model = model_from_json(f.read())
    model.load_weights(model_weights_filename)
    f.close()
    return model

encoder = load_model('encoder_modelLilstm.json', 'encoder_model_weightsLilstm.h5')
decoder = load_model('decoder_modelLilstm.json', 'decoder_model_weightsLilstm.h5')

# encoder = load_model('encoder_model2.json', 'encoder_model_weights2.h5')
# decoder = load_model('decoder_model2.json', 'decoder_model_weights2.h5')

# encoder = load_model('TRYencoder_modelLilstm2.json', 'TRYencoder_model_weightsLilstm2.h5')
# decoder = load_model('TRYdecoder_modelLilstm2.json', 'TRYdecoder_model_weightsLilstm2.h5')

# encoder = load_model('encoder_modelLilstm1.json', 'encoder_model_weightsLilstm1.h5')
# decoder = load_model('decoder_modelLilstm1.json', 'decoder_model_weightsLilstm1.h5')

"""# טעינת TOKENAIZERS"""

import pickle

import joblib
import io
import json

with open('x_tokenizer.pickle', 'rb') as handle:
    loaded_x_tokenizer = pickle.load(handle)

with open('y_tokenizer.pickle', 'rb') as handle:
    loaded_y_tokenizer = pickle.load(handle)

reverse_target_word_index=loaded_y_tokenizer.index_word
reverse_source_word_index=loaded_x_tokenizer.index_word
target_word_index=loaded_y_tokenizer.word_index

reverse_target_word_index[11]

target_word_index['love']

latent_dim=500

"""הופך מSEQ למילים"""

def seq2summary(input_seq):
    newString=''
    for i in input_seq:
      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):
        newString=newString+reverse_target_word_index[i]+' '
    return newString

def seq2text(input_seq):
    newString=''
    for i in input_seq:
      if(i!=0):
        newString=newString+reverse_source_word_index[i]+' '
    return newString

def decode_sequence(input_seq):

    # Encode the input as state vectors.
    e_out, e_h, e_c = encoder.predict(input_seq)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1,1))

    # Chose the 'start' word as the first word of the target sequence
    target_seq[0, 0] = target_word_index['start']

    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder.predict([target_seq] + [e_out, e_h, e_c])
        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])


        sampled_token = reverse_target_word_index[sampled_token_index]



        if(sampled_token!='end'):
            decoded_sentence += ' '+sampled_token



            # Exit condition: either hit max length or find stop word.
        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_summary-1)):
            stop_condition = True



        # Update the target sequence (of length 1).
        target_seq = np.zeros((1,1))
        target_seq[0, 0] = sampled_token_index

        # Update internal states
        e_h, e_c = h, c

    return decoded_sentence

# הדפסה לבדיקה 
for i in range(len(x_val)):
  print("Review:",seq2text(x_val[i]))
  print("Original summary:",seq2summary(y_val[i]))
  print("Predicted summary:",decode_sequence(x_val[i].reshape(1,max_len_text)))
  print("\n")